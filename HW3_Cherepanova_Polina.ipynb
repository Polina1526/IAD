{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvZmnixk7wnO"
   },
   "source": [
    "# Трансформеры\n",
    "В этом домашнем задании мы рассмотим использование трансформеров в библиотеке PyTorch. Рассмотрим задачу языкового моделирования. Попробуем генерировать текст нейронной сетью. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8Yo3-007wnY"
   },
   "source": [
    "Ссылка на данные - https://drive.google.com/drive/folders/1x1A4ElliUGBPnHladGMwPxPuGxI8Vnpu?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:50.368577Z",
     "iopub.status.busy": "2021-12-20T23:29:50.366708Z",
     "iopub.status.idle": "2021-12-20T23:29:52.012786Z",
     "shell.execute_reply": "2021-12-20T23:29:52.011780Z",
     "shell.execute_reply.started": "2021-12-20T23:29:50.368538Z"
    },
    "executionInfo": {
     "elapsed": 7107,
     "status": "ok",
     "timestamp": 1639943538384,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "ilVWZjVc7wna"
   },
   "outputs": [],
   "source": [
    "# хороший тон, импортировать все необходимые библиотеки в одной ячейке ;)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpjfkg2S7wnd"
   },
   "source": [
    "Что такое языковое моделирование? Это предсказание вероятности следующего токена (слова или буквы) на основе предыдущих токенов. Математически это можно описать так:\n",
    "\n",
    "$$P(x_i|x_1, x_2 , ... , x_{i-1})$$ \n",
    "\n",
    "Последовательность $$ x_1, x_2, ... x_{i-1} $$ называют контекстом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Jarc6QD7wnf"
   },
   "source": [
    "## Задание 0 (0 баллов, но сделать нужно)\n",
    "Проставьте знаки неравенств, исходя из вашего опыта:\n",
    "$$ P(раму | мама, мыла) > P(папу | мама, мыла) $$\n",
    "$$ P(столу | дорога, ложка, к) < P(обеду | дорога, ложка, к) $$\n",
    "$$ P(Евпатий | меня, зовут) < P(Ваня | меня, зовут) $$\n",
    "$$ P(журналы | я, часто ,читаю) \\approx P(комиксы | я, часто ,читаю) $$\n",
    "Попробуйте объяснить выбор для каждого из примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUCAmcc17wng"
   },
   "source": [
    "Ответ :\n",
    "\n",
    "1) \"мама мыла раму\" - это устойчивое выражение, а \"мама мыла папу\" - не кажется очень частотным выражением, потому интуитивно вероятность первого будет выше.\n",
    "\n",
    "2) для второго неравенства, аналогичные рассуждения, так как там сравниваются вероятноси для устойчивого выражения и для обычной фразы, которая даже звучит не очень логично для обычной речи.\n",
    "\n",
    "3) Ваня - это более частотное имя, поэтому вероятность фразы \"меня зовут Ваня\" интуитивно будет больше вероятности фразы \"меня зовут Евпатий\"\n",
    "\n",
    "4) в последнем равенстве сложно определить, для меня обе фразы кажутся одинаково частотными, поэтому ставлю примерно равно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgy6l1Cn7wnh"
   },
   "source": [
    "Если для некоторых из примеров проставить знаки достаточно просто, то на некоторые сложно сказать, какой ответ верный. Мы принимаем решение для данного задания исходя их опыта использования русского языка. Мы много читали на русском и слушали огромное количество русской речи. Обучение языковых моделей происходит по схожему принципу. \n",
    "\n",
    "Мы хотим показать модели столько текстов, сколько можем и надеемся, что она наберется достаточно опыта, чтобы расставлять такие знаки неравества максимально схоже с человеком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDqHqD0g7wnj"
   },
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "Мы будем обучать языковую модель для предсказания следущей буквы. Такие языковые модели применяются в распозновании речи, так как предоставляют дополнительную информацию акустической модели при выборе следующего символа. Для начала, откройте файл с данными, посмотрите, какие символы входят в тексты, сколько их. Уберите из текста все символы переноса на новую строку и табуляцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:52.019512Z",
     "iopub.status.busy": "2021-12-20T23:29:52.017700Z",
     "iopub.status.idle": "2021-12-20T23:29:54.593216Z",
     "shell.execute_reply": "2021-12-20T23:29:54.592404Z",
     "shell.execute_reply.started": "2021-12-20T23:29:52.019469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-20 23:29:52--  https://docs.google.com/uc?export=download&id=1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.193.138, 172.217.193.100, 172.217.193.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.193.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0s-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dpjriqga6crjgvdk9i9hkc9hf3pl94jc/1640042925000/09709797766287458306/*/1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-12-20 23:29:53--  https://doc-0s-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dpjriqga6crjgvdk9i9hkc9hf3pl94jc/1640042925000/09709797766287458306/*/1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX?e=download\n",
      "Resolving doc-0s-54-docs.googleusercontent.com (doc-0s-54-docs.googleusercontent.com)... 108.177.11.132, 2607:f8b0:400c:c01::84\n",
      "Connecting to doc-0s-54-docs.googleusercontent.com (doc-0s-54-docs.googleusercontent.com)|108.177.11.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56028286 (53M) [text/plain]\n",
      "Saving to: ‘small_corp_for_test.txt’\n",
      "\n",
      "small_corp_for_test 100%[===================>]  53.43M   197MB/s    in 0.3s    \n",
      "\n",
      "2021-12-20 23:29:54 (197 MB/s) - ‘small_corp_for_test.txt’ saved [56028286/56028286]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ijpKzdj4d1b0zYKot3_380OWtiI-2UJX' -O small_corp_for_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:54.595043Z",
     "iopub.status.busy": "2021-12-20T23:29:54.594760Z",
     "iopub.status.idle": "2021-12-20T23:29:54.952251Z",
     "shell.execute_reply": "2021-12-20T23:29:54.951519Z",
     "shell.execute_reply.started": "2021-12-20T23:29:54.594988Z"
    },
    "executionInfo": {
     "elapsed": 4564,
     "status": "ok",
     "timestamp": 1639943563756,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "QpwVoOpb7wnl",
    "outputId": "adf67180-059e-4b37-8454-5ff90a4e3993",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'small_corp_for_test.txt'\n",
    "file = open(path, 'r')\n",
    "data = file.readlines()\n",
    "file.close()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTdv9NKe8x6A"
   },
   "source": [
    "Посмотрю на 10 случайных элементов файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:54.954001Z",
     "iopub.status.busy": "2021-12-20T23:29:54.953675Z",
     "iopub.status.idle": "2021-12-20T23:29:54.964797Z",
     "shell.execute_reply": "2021-12-20T23:29:54.962209Z",
     "shell.execute_reply.started": "2021-12-20T23:29:54.953962Z"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1639943563757,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "lcQ_As4B7wnm",
    "outputId": "be036e19-2dff-4b72-c800-e2068295af33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ну на прописку буду подавать не я я собираю пакет документов\n",
      "\n",
      "сейчас одну секундочку\n",
      "\n",
      "я предприниматель да для меня самое важное\n",
      "\n",
      "аа икс идея от профи это фоторепортаж\n",
      "\n",
      "стражники хью с треском проламывались сквозь кусты и цепью смыкались внизу чтобы взять беглецов в кольцо\n",
      "\n",
      "аа вы сказали что уже не работаете если\n",
      "\n",
      "через сто двенадцать\n",
      "\n",
      "алло да здравствуйте дмитрий это сергей слышу вас теперь\n",
      "\n",
      "вот и конкуренты\n",
      "\n",
      "кры крыши чтоб тебя\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind = np.random.randint(low=0, high=len(data), size=10)\n",
    "for i in ind:\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:54.966698Z",
     "iopub.status.busy": "2021-12-20T23:29:54.966424Z",
     "iopub.status.idle": "2021-12-20T23:29:59.484592Z",
     "shell.execute_reply": "2021-12-20T23:29:59.483954Z",
     "shell.execute_reply.started": "2021-12-20T23:29:54.966662Z"
    },
    "executionInfo": {
     "elapsed": 5113,
     "status": "ok",
     "timestamp": 1639943568848,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "ucubwkrT-Ocm",
    "outputId": "f806814a-da3b-4f14-ccb9-063b51987a9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409380"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVsNuWVK7zS4"
   },
   "source": [
    "Получается, что в обучающей выборке есть повторяющиеся тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:29:59.487846Z",
     "iopub.status.busy": "2021-12-20T23:29:59.487647Z",
     "iopub.status.idle": "2021-12-20T23:30:02.761486Z",
     "shell.execute_reply": "2021-12-20T23:30:02.760690Z",
     "shell.execute_reply.started": "2021-12-20T23:29:59.487820Z"
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1639943571759,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "niU1b9Cr7yVq",
    "outputId": "a437c3dc-5a26-48b9-b5ab-1c8aacad8556"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_symbols = set()\n",
    "for text in data:\n",
    "    unique_symbols = unique_symbols.union(set(text))\n",
    "len(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:02.763082Z",
     "iopub.status.busy": "2021-12-20T23:30:02.762684Z",
     "iopub.status.idle": "2021-12-20T23:30:02.798774Z",
     "shell.execute_reply": "2021-12-20T23:30:02.798154Z",
     "shell.execute_reply.started": "2021-12-20T23:30:02.763043Z"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1639943571760,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "ltvM5TRB9IcR",
    "outputId": "46ba960e-ff55-4647-83f7-c1dc53876aac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '-',\n",
       " 'а',\n",
       " 'б',\n",
       " 'в',\n",
       " 'г',\n",
       " 'д',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'з',\n",
       " 'и',\n",
       " 'й',\n",
       " 'к',\n",
       " 'л',\n",
       " 'м',\n",
       " 'н',\n",
       " 'о',\n",
       " 'п',\n",
       " 'р',\n",
       " 'с',\n",
       " 'т',\n",
       " 'у',\n",
       " 'ф',\n",
       " 'х',\n",
       " 'ц',\n",
       " 'ч',\n",
       " 'ш',\n",
       " 'щ',\n",
       " 'ъ',\n",
       " 'ы',\n",
       " 'ь',\n",
       " 'э',\n",
       " 'ю',\n",
       " 'я',\n",
       " 'ё'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlaNnWHi93yq"
   },
   "source": [
    "Таким образом, во всех текстах содержатся только символы алфавита, тире, пробела и переноса строки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59wuzeMW-Hez"
   },
   "source": [
    "Уберу из всех тексттов символы \\n и \\t, как просят в задании:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:02.800361Z",
     "iopub.status.busy": "2021-12-20T23:30:02.799970Z",
     "iopub.status.idle": "2021-12-20T23:30:03.375087Z",
     "shell.execute_reply": "2021-12-20T23:30:03.374329Z",
     "shell.execute_reply.started": "2021-12-20T23:30:02.800324Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1639943572391,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "JO4UUl2Q-Q4J"
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('\\n', '').replace('\\t', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gWvdRNv7wnn"
   },
   "source": [
    "## Задание 2 (0.5 балла)\n",
    "Для обучения модели требуется сначала подготовить текст в подходящий для нейросети вид. Важно также отметить, что нужно добавить два токена start и end, которые отвечают за начало и конец текста. Используйте [ и ] для этой задачи. Также нам нужен токен pad, чтобы заполнять им текст до требуемой длинны для формирования батча.\n",
    "\n",
    "Реализуйте метод preprocess класса Preprocessor. Он должен принимать на вход текст и длинну текста, которую мы ожидаем получить на выходе. Текст должен быть переведен в нижний регистр, в конец текста добавляется требуемое число pad токенов, далее текст векторизуется (каждому символу ставится свое число). Вернуть требуется два вектора. Полученный результат без последнего токена (на нем будем обучаться) и полученный результат без первого токена (целевые метки при обучении)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:03.376687Z",
     "iopub.status.busy": "2021-12-20T23:30:03.376431Z",
     "iopub.status.idle": "2021-12-20T23:30:03.386369Z",
     "shell.execute_reply": "2021-12-20T23:30:03.385645Z",
     "shell.execute_reply.started": "2021-12-20T23:30:03.376655Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1639943572393,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "GYkpH5Cf7wno"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.alphabet = '_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '    # не хватало буквы ф\n",
    "        self.token2ind = {}\n",
    "        self.ind2token = {}\n",
    "        for i in range(len(self.alphabet)):\n",
    "            self.token2ind[self.alphabet[i]] = i\n",
    "            self.ind2token[i] = self.alphabet[i]\n",
    "        \n",
    "    \n",
    "    def preprocess(self, text, window_size):\n",
    "        text_preproc = text.lower()\n",
    "        pad_size = window_size - len(text_preproc)\n",
    "        text_preproc += pad_size * '_'\n",
    "\n",
    "        text_vectorized = []\n",
    "        for sym in text_preproc:\n",
    "            text_vectorized.append(self.token2ind[sym])\n",
    "        \n",
    "        return [np.array(text_vectorized[:-1]), np.array(text_vectorized[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af7kJq7J7wnp"
   },
   "source": [
    "## Задание 3 (0.5 балла)\n",
    "Так как мы решили, что текст будет начинаться токеном [ и заканчиваться токеном ], данные нужно поправить. Реализуйте эту идею, добавьте данные токены в ваши тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:03.387569Z",
     "iopub.status.busy": "2021-12-20T23:30:03.387330Z",
     "iopub.status.idle": "2021-12-20T23:30:03.625458Z",
     "shell.execute_reply": "2021-12-20T23:30:03.624763Z",
     "shell.execute_reply.started": "2021-12-20T23:30:03.387534Z"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1639943572833,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "q99H-ZOF7wnq"
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = '[' + data[i] + ']'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCJ4nPjv7wnr"
   },
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "Так как мы не располагаем большими мощностями, то давайте ограничим максимальную длинну текста. Вы можете менять этот порог и тем самым уменьшать кол-во текстов в вашей выборке и увеличивая тем самым скорость обучения. Начнем же мы с 128. \n",
    "Выберите порог и оставьте только те тексты, длина которых не превосходит данный порог.\n",
    "\n",
    "Далее разбейте тексты на train и test, перемешайте тексты при разбиении, размер тестовой выборки должен быть 15% от общего числа текстов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:03.626782Z",
     "iopub.status.busy": "2021-12-20T23:30:03.626542Z",
     "iopub.status.idle": "2021-12-20T23:30:05.745931Z",
     "shell.execute_reply": "2021-12-20T23:30:05.745256Z",
     "shell.execute_reply.started": "2021-12-20T23:30:03.626749Z"
    },
    "executionInfo": {
     "elapsed": 1885,
     "status": "ok",
     "timestamp": 1639943574714,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "XLohAysn7wnr",
    "outputId": "e843849d-f613-44d4-96b0-4941c44dc13d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 128\n",
    "\n",
    "mask = (np.array(list(map(len, data))) <= THRESHOLD)\n",
    "sub_data = np.array(data)[mask]\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBS9tivUGpRi"
   },
   "source": [
    "То есть с данным значением порога получаюм выборку всего из 683438 текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:05.747434Z",
     "iopub.status.busy": "2021-12-20T23:30:05.747123Z",
     "iopub.status.idle": "2021-12-20T23:30:06.486640Z",
     "shell.execute_reply": "2021-12-20T23:30:06.485946Z",
     "shell.execute_reply.started": "2021-12-20T23:30:05.747397Z"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1639943575184,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "OFfczSFBIsr7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:06.488246Z",
     "iopub.status.busy": "2021-12-20T23:30:06.487981Z",
     "iopub.status.idle": "2021-12-20T23:30:08.554065Z",
     "shell.execute_reply": "2021-12-20T23:30:08.553311Z",
     "shell.execute_reply.started": "2021-12-20T23:30:06.488214Z"
    },
    "executionInfo": {
     "elapsed": 1878,
     "status": "ok",
     "timestamp": 1639943577059,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "khOqalLqIuIU"
   },
   "outputs": [],
   "source": [
    "data_train, data_test, _, _ = train_test_split(sub_data, sub_data, test_size=0.15, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7luD45Cj7wns"
   },
   "source": [
    "## Задание 5 (2 балла)\n",
    "Напишем датасет. На вход датасету передается набор текстов, объект класса Preprocessor и размер окна, который вы выбрали в прошлом задании.\n",
    "Реализуйте методы __len__ и __getitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:08.555618Z",
     "iopub.status.busy": "2021-12-20T23:30:08.555361Z",
     "iopub.status.idle": "2021-12-20T23:30:08.564819Z",
     "shell.execute_reply": "2021-12-20T23:30:08.564145Z",
     "shell.execute_reply.started": "2021-12-20T23:30:08.555583Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1639943577060,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "2s7Hh61i7wnt"
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, preproc, win_size = 128):\n",
    "        self.preprocessor = preproc\n",
    "        data = np.array(list(map(lambda text: preproc.preprocess(text, win_size), x)))\n",
    "        self.x = torch.from_numpy(data[:, 0])\n",
    "        self.y = torch.from_numpy(data[:, 1])\n",
    "        self.window_size = win_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:30:08.566424Z",
     "iopub.status.busy": "2021-12-20T23:30:08.566077Z",
     "iopub.status.idle": "2021-12-20T23:31:01.978455Z",
     "shell.execute_reply": "2021-12-20T23:31:01.977659Z",
     "shell.execute_reply.started": "2021-12-20T23:30:08.566388Z"
    },
    "executionInfo": {
     "elapsed": 40859,
     "status": "ok",
     "timestamp": 1639943617912,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "s2WDoi1p7wnt"
   },
   "outputs": [],
   "source": [
    "preproc = Preprocessor()\n",
    "train_dataset = TextDataset(data_train, preproc)\n",
    "test_dataset = TextDataset(data_test, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:01.980079Z",
     "iopub.status.busy": "2021-12-20T23:31:01.979791Z",
     "iopub.status.idle": "2021-12-20T23:31:02.003965Z",
     "shell.execute_reply": "2021-12-20T23:31:02.003253Z",
     "shell.execute_reply.started": "2021-12-20T23:31:01.980043Z"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1639943618497,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "HHTNB9OWdcaN",
    "outputId": "8264da90-b6ed-4fe8-94a7-8a8f8371753f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36, 11, 37, 23,  8, 17, 37,  9, 20, 14, 37, 12,  8, 17, 37,  3,  9,  1,\n",
       "         14, 12, 37, 10,  2, 10,  5,  2, 32, 14, 37, 23, 13, 14,  1,  5, 19, 12,\n",
       "         27, 37, 10,  2, 12,  2, 17,  9, 37, 16, 12,  2, 37, 23, 25, 37, 10,  2,\n",
       "         22, 17, 33, 12, 14, 37, 16, 12,  2, 37,  6,  8,  6, 11, 14, 28, 12,  2,\n",
       "         37,  2, 29, 11,  3,  6, 11, 37,  3,  2, 21, 14, 14, 37, 23,  8, 20, 13,\n",
       "         25, 37,  6,  8,  6, 11, 14, 28, 12,  2, 35,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0]),\n",
       " tensor([11, 37, 23,  8, 17, 37,  9, 20, 14, 37, 12,  8, 17, 37,  3,  9,  1, 14,\n",
       "         12, 37, 10,  2, 10,  5,  2, 32, 14, 37, 23, 13, 14,  1,  5, 19, 12, 27,\n",
       "         37, 10,  2, 12,  2, 17,  9, 37, 16, 12,  2, 37, 23, 25, 37, 10,  2, 22,\n",
       "         17, 33, 12, 14, 37, 16, 12,  2, 37,  6,  8,  6, 11, 14, 28, 12,  2, 37,\n",
       "          2, 29, 11,  3,  6, 11, 37,  3,  2, 21, 14, 14, 37, 23,  8, 20, 13, 25,\n",
       "         37,  6,  8,  6, 11, 14, 28, 12,  2, 35,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQRtmwA7wnu"
   },
   "source": [
    "## Задание 6 (2 балла)\n",
    "Напишем модель. Класс для реализации positional encoding реализован за вас, он нужен, чтобы модель могла после получения эмбедингов понимать, на каком месте какой токен находится.\n",
    "\n",
    "Заполните пропуски в классе модели. Гипперпараметры модели вам предлагается подобрать самостоятельно. Рекомендуется использовать не более 6 слоев в трансформере. В декореде испоьлзуйте две линейных слоя с функцией активации ReLU между ними.\n",
    "\n",
    "## Задание 6_1 (0 баллов, но надо ответить!)\n",
    "При обучении языковой модели на основе трансформеров мы используем маскирование символов (как мы это делаем - уже реализовано). Напишите, почему мы это делаем? Почему это так важно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный приём необходим для корректной работы механизма внимания. Смысл маскирования символов в том, чтобы не дать модели видеть, что будет в будующем. Для этого у матрицы скалярных произведений мы применяем маску, то есть отключаем элементы выше главной диагоняли. Благодаря этому, выход зависит не от всей последовательности целиком, а только он n-1 предыдущих символов при работе с символом под номером n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.005729Z",
     "iopub.status.busy": "2021-12-20T23:31:02.005266Z",
     "iopub.status.idle": "2021-12-20T23:31:02.009337Z",
     "shell.execute_reply": "2021-12-20T23:31:02.008449Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.005693Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639943618498,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "Zd7xTLJifinR"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.011586Z",
     "iopub.status.busy": "2021-12-20T23:31:02.011069Z",
     "iopub.status.idle": "2021-12-20T23:31:02.020754Z",
     "shell.execute_reply": "2021-12-20T23:31:02.020054Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.011549Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1639943618500,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "YsqhLzYi7wnv"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.022826Z",
     "iopub.status.busy": "2021-12-20T23:31:02.022282Z",
     "iopub.status.idle": "2021-12-20T23:31:02.033285Z",
     "shell.execute_reply": "2021-12-20T23:31:02.032577Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.022763Z"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1639955313767,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "ALiXImrC7wnw"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.hidden_layer = 512\n",
    "        self.emb = nn.Embedding(vocab_size, self.hidden_layer)\n",
    "        self.pe = PositionalEncoding(self.hidden_layer)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(self.hidden_layer, nhead=1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=6)\n",
    "        self.lin_layer_size = 1024\n",
    "        self.decoder = nn.Sequential(nn.Linear(self.hidden_layer, self.lin_layer_size),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.lin_layer_size, vocab_size))\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.pe(self.emb(x))\n",
    "        x = x.transpose(1, 0)\n",
    "        x = self.transformer_encoder_layer(x, src_mask)\n",
    "        x = self.decoder(x)\n",
    "        return x.transpose(1, 0)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # А вот и то самое маскирование\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.035055Z",
     "iopub.status.busy": "2021-12-20T23:31:02.034343Z",
     "iopub.status.idle": "2021-12-20T23:31:02.197461Z",
     "shell.execute_reply": "2021-12-20T23:31:02.196733Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.035002Z"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1639955318018,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "1MdDdObX7wnw"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len('_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQwGiVxj7wnx"
   },
   "source": [
    "## Задание 7 (2,5 балла)\n",
    "Финишная прямая. Давайте реализуем класс для обучения модели и ее валидации. Следуйте указаниям в коде и заполните недостающие фрагменты в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.201274Z",
     "iopub.status.busy": "2021-12-20T23:31:02.201078Z",
     "iopub.status.idle": "2021-12-20T23:31:02.221802Z",
     "shell.execute_reply": "2021-12-20T23:31:02.221186Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.201250Z"
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1639955320293,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "ovb-mCuW7wnx"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, train_dataset, test_dataset):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.train_batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        \n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                            batch_size = self.train_batch_size,\n",
    "                                                            shuffle = True,\n",
    "                                                            pin_memory = True)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                            batch_size = self.test_batch_size,\n",
    "                                                            shuffle = True,\n",
    "                                                            pin_memory = True)\n",
    "        self.train_dataloader_size = len(self.train_dataloader)\n",
    "        self.test_dataloader_size = len(self.test_dataloader)\n",
    "        \n",
    "        self.device = 'cuda:0'\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "                             # используйте CrossEntrophyLoss, передайте в качетсве параметра \n",
    "                             # ignore index индекс символа _, чтобы модель не штрафовалась за то\n",
    "                             # что идет после закрывающего токена\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.steps_to_print = 1000\n",
    "        \n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        \n",
    "        for batch in self.train_dataloader:\n",
    "            x, y = batch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            \n",
    "            # реализуйте шаги обучения модели\n",
    "            # сохраняйте значение ошибки в переменную counted_loss\n",
    "            scr_mask = self.model.generate_square_subsequent_mask(THRESHOLD - 1).to(self.device)\n",
    "            predicted = self.model(x, scr_mask)\n",
    "            loss = self.criterion(predicted.transpose(1, 2), y)\n",
    "            counted_loss += loss\n",
    "            step += 1\n",
    "            it += 1\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if step%self.steps_to_print == 0:\n",
    "                result = 'Train epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "    \n",
    "    def validate_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        for batch in self.test_dataloader:\n",
    "            x, y = batch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            \n",
    "            # реализуйте шаги для теста модели\n",
    "            # помните, что данный метод уже запускается из \n",
    "            # блока with torch.no_grad(), а потому \n",
    "            # повторно его использовать не нужно\n",
    "            scr_mask = self.model.generate_square_subsequent_mask(THRESHOLD - 1).to(self.device)\n",
    "            predicted = self.model(x, scr_mask)\n",
    "            loss = self.criterion(predicted.transpose(1, 2), y)\n",
    "            counted_loss += loss\n",
    "            step += 1\n",
    "            it += 1\n",
    "            \n",
    "            if step%(self.steps_to_print//2) == 0:\n",
    "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "        \n",
    "    def train(self, number_of_epochs):\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(1, number_of_epochs+1):\n",
    "            self.model.train()\n",
    "            self.train_one_epoch(epoch)\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                self.validate_one_epoch(epoch)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJlgZb8S7wnz"
   },
   "source": [
    "Что такое ppl? Перплексия. Ее можно интерпретировать как меру \"удивленности\" модели нужному символу. Чем меньше данная величина, тем лучше, ведь это значит, что модель если и сделала неправильный выбор, то не сильно удивлена своей ошибке.\n",
    "\n",
    "Проведите несколько экспериментов, посмотрите, при каких гипперпараметрах значение перплексии минимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELTW2pQq7wnz"
   },
   "source": [
    "## Задание 8 (0.5 балла)\n",
    "Запустите обучение на нескольких эпохах. Ориентируйтесь на ваши вычислительные мощности и время работы. Вы всегда можете посчитать, сколько секунд уходит на один батч."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:31:02.223626Z",
     "iopub.status.busy": "2021-12-20T23:31:02.223213Z",
     "iopub.status.idle": "2021-12-20T23:59:54.857804Z",
     "shell.execute_reply": "2021-12-20T23:59:54.856992Z",
     "shell.execute_reply.started": "2021-12-20T23:31:02.223590Z"
    },
    "executionInfo": {
     "elapsed": 3434350,
     "status": "ok",
     "timestamp": 1639958756818,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "VuIFyrYf7wn0",
    "outputId": "6ffd2dcb-4cab-4cb7-dec0-eebc3c39c637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 | Step 1000/9077 | Counted loss tensor(2501.7603, device='cuda:0', grad_fn=<AddBackward0>) | ppl 12.20395992132023 | time 36.480594873428345 | \n",
      "Train epoch 1 | Step 2000/9077 | Counted loss tensor(2403.5393, device='cuda:0', grad_fn=<AddBackward0>) | ppl 11.062261150372649 | time 35.63796281814575 | \n",
      "Train epoch 1 | Step 3000/9077 | Counted loss tensor(2356.8708, device='cuda:0', grad_fn=<AddBackward0>) | ppl 10.557862992492023 | time 35.75729966163635 | \n",
      "Train epoch 1 | Step 4000/9077 | Counted loss tensor(2319.9475, device='cuda:0', grad_fn=<AddBackward0>) | ppl 10.175142331391571 | time 35.70889067649841 | \n",
      "Train epoch 1 | Step 5000/9077 | Counted loss tensor(2294.1814, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.916317030881677 | time 35.89362668991089 | \n",
      "Train epoch 1 | Step 6000/9077 | Counted loss tensor(2279.5198, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.771986739170137 | time 36.01641869544983 | \n",
      "Train epoch 1 | Step 7000/9077 | Counted loss tensor(2262.7319, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.609306240500947 | time 36.030949115753174 | \n",
      "Train epoch 1 | Step 8000/9077 | Counted loss tensor(2247.9592, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.468394669778702 | time 35.9285843372345 | \n",
      "Train epoch 1 | Step 9000/9077 | Counted loss tensor(2239.8362, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.391792935798797 | time 35.95180630683899 | \n",
      "Validate epoch 1 | Step 500/1602 | Counted loss tensor(1087.0406, device='cuda:0') | ppl 8.794102492665639 | time 5.944292783737183 | \n",
      "Validate epoch 1 | Step 1000/1602 | Counted loss tensor(1084.9634, device='cuda:0') | ppl 8.757643674102031 | time 5.945891618728638 | \n",
      "Validate epoch 1 | Step 1500/1602 | Counted loss tensor(1085.0261, device='cuda:0') | ppl 8.758742023061076 | time 5.937030076980591 | \n",
      "\n",
      "Train epoch 2 | Step 1000/9077 | Counted loss tensor(2230.7688, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.307019673041198 | time 35.97015571594238 | \n",
      "Train epoch 2 | Step 2000/9077 | Counted loss tensor(2222.0732, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.226440380761225 | time 35.922954082489014 | \n",
      "Train epoch 2 | Step 3000/9077 | Counted loss tensor(2215.5000, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.165992027978858 | time 35.88776969909668 | \n",
      "Train epoch 2 | Step 4000/9077 | Counted loss tensor(2210.5562, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.120788591296808 | time 35.86732339859009 | \n",
      "Train epoch 2 | Step 5000/9077 | Counted loss tensor(2206.0229, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.079534981667269 | time 35.80546689033508 | \n",
      "Train epoch 2 | Step 6000/9077 | Counted loss tensor(2200.1582, device='cuda:0', grad_fn=<AddBackward0>) | ppl 9.026442792261237 | time 36.045616149902344 | \n",
      "Train epoch 2 | Step 7000/9077 | Counted loss tensor(2196.0215, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.989179325713682 | time 35.83732318878174 | \n",
      "Train epoch 2 | Step 8000/9077 | Counted loss tensor(2191.4292, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.947994031455934 | time 35.914371490478516 | \n",
      "Train epoch 2 | Step 9000/9077 | Counted loss tensor(2189.6057, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.931690769801332 | time 36.17913317680359 | \n",
      "Validate epoch 2 | Step 500/1602 | Counted loss tensor(1061.0475, device='cuda:0') | ppl 8.348610418202846 | time 6.033084869384766 | \n",
      "Validate epoch 2 | Step 1000/1602 | Counted loss tensor(1057.2660, device='cuda:0') | ppl 8.285707096823394 | time 5.938710927963257 | \n",
      "Validate epoch 2 | Step 1500/1602 | Counted loss tensor(1058.6246, device='cuda:0') | ppl 8.308252105084563 | time 5.959117412567139 | \n",
      "\n",
      "Train epoch 3 | Step 1000/9077 | Counted loss tensor(2183.5564, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.877824663804418 | time 36.12002158164978 | \n",
      "Train epoch 3 | Step 2000/9077 | Counted loss tensor(2181.6230, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.860677424521077 | time 36.16879844665527 | \n",
      "Train epoch 3 | Step 3000/9077 | Counted loss tensor(2178.1643, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.830083598428068 | time 36.13176965713501 | \n",
      "Train epoch 3 | Step 4000/9077 | Counted loss tensor(2174.7556, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.800035993896296 | time 36.05591154098511 | \n",
      "Train epoch 3 | Step 5000/9077 | Counted loss tensor(2171.8428, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.774438805273588 | time 36.157174825668335 | \n",
      "Train epoch 3 | Step 6000/9077 | Counted loss tensor(2170.0935, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.759103297212876 | time 36.098954916000366 | \n",
      "Train epoch 3 | Step 7000/9077 | Counted loss tensor(2167.3357, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.734980827451317 | time 36.07501029968262 | \n",
      "Train epoch 3 | Step 8000/9077 | Counted loss tensor(2168.3486, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.743834200824315 | time 36.04248094558716 | \n",
      "Train epoch 3 | Step 9000/9077 | Counted loss tensor(2163.8560, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.704638369519246 | time 36.03596782684326 | \n",
      "Validate epoch 3 | Step 500/1602 | Counted loss tensor(1045.1443, device='cuda:0') | ppl 8.087249123507524 | time 5.941265344619751 | \n",
      "Validate epoch 3 | Step 1000/1602 | Counted loss tensor(1047.1860, device='cuda:0') | ppl 8.12034200794344 | time 5.950422286987305 | \n",
      "Validate epoch 3 | Step 1500/1602 | Counted loss tensor(1045.8052, device='cuda:0') | ppl 8.097945857448378 | time 5.937107801437378 | \n",
      "\n",
      "Train epoch 4 | Step 1000/9077 | Counted loss tensor(2154.9197, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.62719879125561 | time 36.103004455566406 | \n",
      "Train epoch 4 | Step 2000/9077 | Counted loss tensor(2159.2983, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.66505629482336 | time 36.03577184677124 | \n",
      "Train epoch 4 | Step 3000/9077 | Counted loss tensor(2158.6946, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.659826988227973 | time 36.11110830307007 | \n",
      "Train epoch 4 | Step 4000/9077 | Counted loss tensor(2156.3035, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.639145212474295 | time 36.12978959083557 | \n",
      "Train epoch 4 | Step 5000/9077 | Counted loss tensor(2150.8057, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.591778105698978 | time 36.145020723342896 | \n",
      "Train epoch 4 | Step 6000/9077 | Counted loss tensor(2153.4482, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.614513036217742 | time 36.023279428482056 | \n",
      "Train epoch 4 | Step 7000/9077 | Counted loss tensor(2152.8184, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.609088446868268 | time 35.94033098220825 | \n",
      "Train epoch 4 | Step 8000/9077 | Counted loss tensor(2151.4304, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.597148743569514 | time 35.993041038513184 | \n",
      "Train epoch 4 | Step 9000/9077 | Counted loss tensor(2148.5388, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.572323601549186 | time 36.056257247924805 | \n",
      "Validate epoch 4 | Step 500/1602 | Counted loss tensor(1038.9567, device='cuda:0') | ppl 7.987785182936676 | time 5.975192546844482 | \n",
      "Validate epoch 4 | Step 1000/1602 | Counted loss tensor(1037.8286, device='cuda:0') | ppl 7.969783815918296 | time 5.93863320350647 | \n",
      "Validate epoch 4 | Step 1500/1602 | Counted loss tensor(1036.8583, device='cuda:0') | ppl 7.9543316384992755 | time 5.946001052856445 | \n",
      "\n",
      "Train epoch 5 | Step 1000/9077 | Counted loss tensor(2145.4255, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.545677145639765 | time 36.0689058303833 | \n",
      "Train epoch 5 | Step 2000/9077 | Counted loss tensor(2144.0454, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.533892528503506 | time 36.05077338218689 | \n",
      "Train epoch 5 | Step 3000/9077 | Counted loss tensor(2144.5500, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.5381989098146 | time 36.086968421936035 | \n",
      "Train epoch 5 | Step 4000/9077 | Counted loss tensor(2136.4490, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.469310484715557 | time 36.05907487869263 | \n",
      "Train epoch 5 | Step 5000/9077 | Counted loss tensor(2142.9800, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.524804571344596 | time 36.03024506568909 | \n",
      "Train epoch 5 | Step 6000/9077 | Counted loss tensor(2140.2253, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.501353706987821 | time 36.10803556442261 | \n",
      "Train epoch 5 | Step 7000/9077 | Counted loss tensor(2142.3916, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.51978990646128 | time 36.06541872024536 | \n",
      "Train epoch 5 | Step 8000/9077 | Counted loss tensor(2137.9609, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.482126197519896 | time 36.034040451049805 | \n",
      "Train epoch 5 | Step 9000/9077 | Counted loss tensor(2136.6133, device='cuda:0', grad_fn=<AddBackward0>) | ppl 8.470701856022515 | time 35.96872544288635 | \n",
      "Validate epoch 5 | Step 500/1602 | Counted loss tensor(1032.1506, device='cuda:0') | ppl 7.8797899889417256 | time 5.942445516586304 | \n",
      "Validate epoch 5 | Step 1000/1602 | Counted loss tensor(1032.4653, device='cuda:0') | ppl 7.884751287359224 | time 5.941402196884155 | \n",
      "Validate epoch 5 | Step 1500/1602 | Counted loss tensor(1029.7585, device='cuda:0') | ppl 7.8421822602884355 | time 5.932741165161133 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_dataset, test_dataset)\n",
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19aJR1N7wn0"
   },
   "source": [
    "## Задание 9 (1 балл)\n",
    "Итак, давайте попробуем погенерировать текст нашей сеткой. Закончите функцию по генерации текста. Попробуйте сгенерировать какой-нибудь текст. Помните, что если вы хотите генерировать текст с нуля, то вы должны передать в качестве текста только токен start.\n",
    "Прекратите генерировать текст, если модель выдала токен end или длинна текста больше 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:54.859653Z",
     "iopub.status.busy": "2021-12-20T23:59:54.859218Z",
     "iopub.status.idle": "2021-12-20T23:59:54.867252Z",
     "shell.execute_reply": "2021-12-20T23:59:54.866528Z",
     "shell.execute_reply.started": "2021-12-20T23:59:54.859614Z"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1639958785461,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "RQB8vrDo7wn0"
   },
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    x = []\n",
    "    \n",
    "    for letter in text:\n",
    "        x.append(preproc.token2ind[letter])\n",
    "    x = torch.from_numpy(np.array(x)).unsqueeze(0).to('cuda:0')\n",
    "    \n",
    "    pred = model(x, model.generate_square_subsequent_mask(len(text)).to('cuda:0'))\n",
    "    ind = torch.argmax(pred[0][-1]).item()\n",
    "    \n",
    "    text += preproc.ind2token[ind] \n",
    "    \n",
    "    if (len(text) > 150) or (text[-1] == ']'):\n",
    "        return text\n",
    "    else:\n",
    "        return generate_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:54.868680Z",
     "iopub.status.busy": "2021-12-20T23:59:54.868371Z",
     "iopub.status.idle": "2021-12-20T23:59:55.109502Z",
     "shell.execute_reply": "2021-12-20T23:59:55.108855Z",
     "shell.execute_reply.started": "2021-12-20T23:59:54.868643Z"
    },
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1639960053214,
     "user": {
      "displayName": "Полина Черепанова",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05960195260329054797"
     },
     "user_tz": -180
    },
    "id": "8Vc7_5lFWDVW",
    "outputId": "7d80da22-45a8-4069-c5af-64e9737ef787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[слововот вот т т вот то вот тако вот вот вот вот вот вот такот вот вот вот вот вот вот т ворот такот вот вот вот вот вот вот вот т такорот вот вот вот'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('[слово')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:55.110997Z",
     "iopub.status.busy": "2021-12-20T23:59:55.110732Z",
     "iopub.status.idle": "2021-12-20T23:59:55.292483Z",
     "shell.execute_reply": "2021-12-20T23:59:55.291665Z",
     "shell.execute_reply.started": "2021-12-20T23:59:55.110958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[начало вено про посто посто по подато пому по поду пода пода пововоду поду поду поду поду поду поду поду пода пода пода пода пода пода пода пода поска'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('[начало')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:55.294448Z",
     "iopub.status.busy": "2021-12-20T23:59:55.294156Z",
     "iopub.status.idle": "2021-12-20T23:59:55.474606Z",
     "shell.execute_reply": "2021-12-20T23:59:55.473760Z",
     "shell.execute_reply.started": "2021-12-20T23:59:55.294394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[полина пость получать по по податому по податому по по податому по по податому потому что что во нисто нисто ниму ниму на на по по по по по пому по по'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('[полина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:55.476320Z",
     "iopub.status.busy": "2021-12-20T23:59:55.476058Z",
     "iopub.status.idle": "2021-12-20T23:59:55.699034Z",
     "shell.execute_reply": "2021-12-20T23:59:55.698304Z",
     "shell.execute_reply.started": "2021-12-20T23:59:55.476286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[учёба по пододоват ват поду поду пододолнить вать по по поду поду пода пода пода пода подоска пода подоска подоска повововоду поду подуму подуметотото'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('[учёба')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T23:59:55.702127Z",
     "iopub.status.busy": "2021-12-20T23:59:55.701555Z",
     "iopub.status.idle": "2021-12-20T23:59:55.964808Z",
     "shell.execute_reply": "2021-12-20T23:59:55.964142Z",
     "shell.execute_reply.started": "2021-12-20T23:59:55.702097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[выхухоль сте подить поде подить подать пода пода пода пода пода пода пода пода пода пода пода пода пода полуска полуска пода пожа пожа пожа пожа пожа '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('[выхухоль')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, что модель очень любит слоги 'по', но, тем не менее, иногда выдаёт что-то похожее на слова.\n",
    "\n",
    "Я подбирала различные гиперпараметры, такие как ширина скрытых слоёв, количество скрытых слоёв и темп обучения. При очень широких скрытых слоях, модели нужно было много эпох для обучения и это занимало очень много времени, при маленькой ширине получалась очень плохая модель, она генерировала только пробелы или только один слог 'по' много раз подряд. Поэтому пришлось подобрать оптимальную ширину, такую что, модель уже научилась генерировать хоть какие-то слова и при этом училасть всего чуть меньше двух часов. Для этой задачи подходит очень небольшой тесп обучения, поэтому я выбрала 0.0001, хотя 0.00001 и меньше могут давать лучшее качество, но тогда модель очень долго учится. Было бы неплохо подобрать шедулер, чтобы он уменьшал тесм обучения, но на это мне, к сожалению, не хватило времени, уже время поджимало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LrNWsxm7wn1"
   },
   "source": [
    "## Задание 10* (Задание - бонус, 5 баллов за реализацию при условии, что сделаны прошлые задания)\n",
    "Давайте вспомним, что такое transfer learning. Мы хотим использовать уже предобученные эмбединги для нашей сети, чтобы наша сеть обучалась быстрее. Давайте попробуем обучить новую модель на уровне слов, а не символов, но для упрощения задачи используем предобученный слой из библиотеки Natasha, а вернее, ее блок Navec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaPxnptb7wn1"
   },
   "source": [
    "[Изучите](https://github.com/natasha/navec) то, как вставить слой в вашу нейронную сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjrHoVeC7wn2"
   },
   "source": [
    "Теперь мы хотим, чтобы на вход модели подавались слова, модифицируйте ваш датасет. Возвращайте теперь номер слова в словаре navec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGd99Kym7wn2"
   },
   "outputs": [],
   "source": [
    "class TextDataset_Navec(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, win_size = 128):\n",
    "        # YOUR CODE HERE\n",
    "        self.navec = ...\n",
    "        ################\n",
    "    \n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        ################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE\n",
    "        ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRBk05bQ7wn2"
   },
   "source": [
    "Немного модифицируем модель. Теперь нам не нужны слои с трансформером, так как весь механизм внимания уже заложен в ембедингах. Давайте попробуем просто пройтись линейной головой над эмбедингами. Выберите параметры самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfF3PSD47wn3"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.emb_navec = ...\n",
    "        self.head = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = ... # emb\n",
    "        x = ... # head\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n55j7w767wn3"
   },
   "source": [
    "Теперь дело за малым! Надо немного модифицировать класс обучения, так как мы не используем маскирование, после чего можно приступить к тесту!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abQj5lOz7wn3"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, train_dataset, test_dataset):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.train_batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        \n",
    "        self.train_dataloader = ...\n",
    "        self.test_dataloader = ...\n",
    "        self.train_dataloader_size = ...\n",
    "        self.test_dataloader_size = ...\n",
    "        \n",
    "        self.device = 'cuda:0'\n",
    "        self.criterion = ... \n",
    "        \n",
    "        self.optimizer = ...\n",
    "        \n",
    "        self.steps_to_print = 1000\n",
    "        \n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        \n",
    "        for batch in self.train_dataloader:\n",
    "            x, y = batch\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # реализуйте шаги обучения модели\n",
    "            # сохраняйте значение ошибки в переменную counted_loss\n",
    "            \n",
    "            ################\n",
    "            \n",
    "            \n",
    "            if step%self.steps_to_print == 0:\n",
    "                result = 'Train epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "    \n",
    "    def validate_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        for batch in self.test_dataloader:\n",
    "            x, y = batch\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # реализуйте шаги для теста модели\n",
    "            # помните, что данный метод уже запускается из \n",
    "            # блока with torch.no_grad(), а потому \n",
    "            # повторно его использовать не нужно\n",
    "            \n",
    "            ################\n",
    "            \n",
    "            if step%(self.steps_to_print//2) == 0:\n",
    "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "        \n",
    "    def train(self, number_of_epochs):\n",
    "        model.to(self.device)\n",
    "        for epoch in range(1, number_of_epochs+1):\n",
    "            model.train()\n",
    "            self.train_one_epoch(epoch)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                self.validate_one_epoch(epoch)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSWgmo9s7wn4"
   },
   "source": [
    "Запустите обучение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV2clj6e7wn5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
